%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tufte-Style Book (Minimal Template)
% LaTeX Template
% Version 1.0 (5/1/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% IMPORTANT NOTE:
% In addition to running BibTeX to compile the reference list from the .bib
% file, you will need to run MakeIndex to compile the index at the end of the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{tufte-book} % Use the tufte-book class which in turn uses the tufte-common class

\hypersetup{colorlinks} % Comment this line if you don't wish to have colored links

\usepackage{microtype} % Improves character and word spacing

\usepackage{lipsum} % Inserts dummy text

\usepackage{booktabs} % Better horizontal rules in tables
\usepackage{arydshln}

\usepackage{graphicx} % Needed to insert images into the document
\graphicspath{{graphics/}} % Sets the default location of pictures
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio} % Improves figure scaling

\usepackage{fancyvrb} % Allows customization of verbatim environments
\fvset{fontsize=\normalsize} % The font size of all verbatim text can be changed here
\usepackage[vlined, linesnumbered, ruled, lined, boxed, commentsnumbered]{algorithm2e}
\newcommand\mycommfont[1]{\textcolor{gray}{#1}}
\SetCommentSty{mycommfont}
\usepackage{xspace} % Used for printing a trailing space better than using a tilde (~) using the \xspace command
\usepackage{amsmath}
\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage} % Command to insert a blank page
\newcommand{\apriori}{\textit{a. priori \xspace}}
\newcommand{\eg}{\textit{e. g.}\xspace}
\newcommand{\ie}{\textit{i. e}\xspace}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\subsectionbf}[1]{\subsection{\textbf{#1}}}
\newcommand{\sectionbf}[1]{\section{\textbf{#1}}}
\newcommand{\figref}[1]{Fig.\xspace\ref{#1}\xspace}
\newcommand{\tableref}[1]{Table \xspace\ref{#1}\xspace}
\newcommand{\sref}[1]{\sidenote{See page \xspace \pageref{#1} \xspace}}
\newcommand{\eqnref}[1]{Eq.\xspace\ref{#1}\xspace}
\newcommand{\algref}[1]{Alg.\xspace\ref{#1}\xspace}
\newcommand{\Prob}{\text{P}}
\newcommand{\Expec}{\text{E}}
\newcommand{\qvec}{\mathbf{q}}
\newcommand{\uvec}{\mathbf{u}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\Proj}{\text{Proj}}
\newcommand{\InvProj}{\text{Proj}^{-1}}


\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}

%----------------------------------------------------------------------------------------
%	BOOK META-INFORMATION
%----------------------------------------------------------------------------------------

\title{Articulated 3D SLAM} % Title of the book

\author{Matthew Klingensmith} % Author

%\publisher{Publisher Name} % Publisher

%----------------------------------------------------------------------------------------

\begin{document}

\frontmatter

%----------------------------------------------------------------------------------------
%	EPIGRAPH 
%----------------------------------------------------------------------------------------

% \thispagestyle{empty}
% \openepigraph{Quotation 1}{Author, {\itshape Source}}
% \vfill
% \openepigraph{Quotation 2}{Author}
% \vfill
% \openepigraph{Quotation 3}{Author}

%----------------------------------------------------------------------------------------

\maketitle % Print the title page

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

% \newpage
% \begin{fullwidth}
% ~\vfill
% \thispagestyle{empty}
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{\baselineskip}
% Copyright \copyright\ \the\year\ \thanklessauthor
% 
% \par\smallcaps{Published by \thanklesspublisher}
% 
% \par\smallcaps{\url{http://www.bookwebsite.com}}
% 
% \par License information.\index{license}
% 
% \par\textit{First printing, \monthyear}
% \end{fullwidth}

%----------------------------------------------------------------------------------------

\tableofcontents % Print the table of contents

%----------------------------------------------------------------------------------------

%\listoffigures % Print a list of figures

%----------------------------------------------------------------------------------------

%\listoftables % Print a list of tables

%----------------------------------------------------------------------------------------
%	DEDICATION PAGE
%----------------------------------------------------------------------------------------

% \cleardoublepage
% ~\vfill
% \begin{doublespace}
% \noindent\fontsize{18}{22}\selectfont\itshape
% \nohyphenation
% Dedicated to my family and friends.
% \end{doublespace}
% \vfill
% \vfill

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

%\cleardoublepage
\chapter*{Introduction}
To interact with and understand the world, robots must interpret and fuse noisy and ambiguous signals from their sensors. Consider the case of a multi-jointed robot arm with a 2D or 3D camera attached to its end effector (\figref{fig:ada_cup}). The robot receives noisy signals from its joint encoders, the hand-mounted camera, and other sensors. How can the robot make sense of the world given these noisy, heterogenous signals? How can it then take action based on this information? To successfully complete a task (such as grasping an object), it is critical that the robot have a consistent and accurate geometric representation of the world around it. 

\begin{marginfigure}
\includegraphics[width=1.0\textwidth]{img/ada_cup}
\caption{The \textsc{Ada} robot with hand-mounted camera picking up a cup. }
\label{fig:ada_cup}
\end{marginfigure}

Dense 3D reconstruction is a well-studied problem in computer vision and robotics. Given a series of noisy sensor readings a unknown poses, the task is to infer the true geometric structure and color of the world. This  is a variant of the Simultaneous Localization and Mapping (SLAM) problem \sref{sec:slam}, since it requires simultaneously estimating the state of the sensor and the scene at once. In the context of robotic manipulation, dense 3D reconstruction is useful for scene understanding, planning, grasping, and more. 

Typical techniques for solving this problem focus on handheld sensors, and thus assume that little is known about the pose of the sensor aside from weak priors on the magnitude of the motion (though it is sometimes assumed that other sources of motion estimation, such as markers, gyroscopes, or inertial measurement units are available). For this reason, many prior works directly estimate the pose of a handheld sensor using visual (or depth) data alone.  Depending on the type of sensor, the lack of additional information can lead to inaccuracies in pose and ultimately  in the resulting 3D reconstruction. For instance, using a handheld depth amera, it is impossible to detect motion parallel to a flat plane using the (unchanging) depth data alone \sref{sec:dense_slam}, without additional sensors or \apriori knowledge of how the sensor moves.

On the other hand, robot linkages provide extremely strong cues of the sensor's motion, due to their constrained kinematics \sref{sec:robot_kinematics}. As the robot actuates its joints, the sensor moves in a predictable manner. If the joint angles of the robot are perfeclty known, and the robot is perfectly rigid, the pose of the sensor is known with absolute certainty through forward kinematics. In this context, dense 3D reconstruction is a simple mapping problem rather than a full SLAM problem.

However, even articulated robotic linkages may have significant noise and systematic inaccuracies \sref{sec:encoder_noise}. The use of relative joint encoders leads to inaccuracy that compounds over time; and on some systems, joint encoders are seperated from the actuation system by a mechanism with unpredictable dynamics.  The \textit{Baxter} (\figref{fig:baxter}) robot, for instance, has series-elastic actuators and deformable plastic links which significantly increase its kinematic uncertainty. Another robot arm, the \textit{Barret WAM}, is driven by cables. Stretch in the cables results in systematic error that is configuration-dependant. In the extreme case, there may be degrees of freedom that are entirely unknown (such as the pose of the robot's base, or the extrinsic calibration of the sensor). One way of dealing with systematic error  is to calibrate the joint encoders to account for unknown dynamics and other sources of systematic error; but if additional unmodeled dynamics are introduced (such as a grasped object, or exterior forces),  the calibration will fail to capture them.

\begin{marginfigure}
\includegraphics{img/baxter_hand.png}
\caption{A camera mounted on the hand of a \textit{Rethink Robotics} Baxter robot.}
\label{fig:baxter}
\end{marginfigure}


Uncertainty and unmodeled systematic error can be corrected simultaneously by fusing visual data from the camera with the robot's joint encoder data. If the camera is mounted externally, so that all or part of the robot is visible, the problem becomes the well-studied \textit{articulated tracking} \sref{sec:articulated_tracking} problem. In this case, the robot's known kinematic model can be used in conjunction with visual sensor data to directly estimate the robot's configuration. However, this work is concerned primarily with \textit{hand-mounted} sensors which, for the most part, cannot see the actuator on which they're mounted. In this case, the localization algorithm can't rely on a known model of the world to estimate the robot's configuration. Without this information, is it still possible to estimate the robot's configuration?

This work proposes a method of simultaneously estimating the joint angles of a robot and a dense 3D map of the world around it by solving a constrained visual SLAM problem \sref{sec:kin_slam}. Like in the traditional 3D SLAM problem, this method assumes no \apriori knowledge of the scene, and will have to take the sensor's pose uncertainty into account. Like the articulated tracking problem, it leverages the known kinematic structure of the robot to strongly inform the sensor's pose.

 In some ways, articulated SLAM is easier than the direct 6DOF SLAM problem, because the pose of the sensor is so tightly constrained by the robot's kinematics. However, since the inverse mapping between the sensor's pose and the robot's joint angles is nonlinear and underconstrained, translating 3D SLAM techniques to this domain will require work.
 
This work includes a brief overview of theory and related works \sref{chap:background}, an outline of potential methods of solving the articulated SLAM problem \sref{chap:framework},  2D and 3D simulations and experiments \sref{chap:experiments} involving robots with depth sensors, and an outline of future work \sref{chap:research}.

\chapter{Background}\label{chap:background}
Consider the use case of a robot arm with an attached 2D or 3D camera. As the robot adctuates its joints, the algorithm must use the (noisy, incomplete) data from the camera in conjunction with the robot's (noisy, biased) joint encoders to simultaneously localize the robot and reconstruct a geometric representation of the robot's workspace. Solving this problem requires an understanding of the robot's kinematics, the mechanics of joint encoders, the characteristics of the sensor, and SLAM techniques.

\sectionbf{Robot Kinematics}
\label{sec:robot_kinematics}
A \textit{kinematic linkage}\cite{Mason2001} consists of a series of rigid bodies (called \textit{links}) attached to one another through mechanisms (called \textit{joints}) that constrain their motion. A joint has between 1 and 6 degrees of freedom (DOF) which define how it constrains the motion of its attached links. For instance, a rotary joint has one degree of freedom that defines a pure rotation around an axis, while a ball joint has two to three degrees of freedom defining a pure rotation around 2 or more axes. The joint which constrains link $A$ to link $B$, and which has configuration $q_i$ has the transformation:

\begin{equation}
T^{A}_{B}(q_i)\in SE(3)
\end{equation}

\noindent and as $q_i$ changes, so does the transformation between links $A$ and $B$.

A kinematic linkage can be represented as an undirected graph where vertexes are links, and the edges are joints between pairs of links. When the graph is acyclic, the linkage is said to be a \textit{kinematic tree}. For instance, a robot with two arms, a head, and a fixed base has a kinematic tree with three branches: one for each arm, and one for the head. Any link may be treated arbitrarily as the root of the tree.

The transformation of any link $L_i$ with respect to a fixed reference frame $W$ can be calculated by traversing the kinematic tree and appending the transformations implied by each joint from the root of the tree (a process called \textit{forward kinematics}):

\begin{equation}
T^{W}_{L_i} = T^{L_{i - 1}}_{L_{i}}(q_{i - 1}) \ldots T^{L_1}_{L_2} (q_1)  T^{W}_{L_1}
\end{equation}

\noindent the path from the root to the link in question is called that link's \textit{kinematic chain}. The end of the kinematic chain is called the \textit{end effector}. This work will use the reference frame of the camera as the end of the arm's kinematic chain.

A robot's configuration $\qvec \in \mathbb{R^N}$ is a vector which concatenates all of its joints' degrees of fredom:

\begin{equation}
\qvec = \colvec{3}{q_1}{\vdots}{q_N}
\end{equation}

The partial derivative of link $i$'s reference frame with respect to $\qvec$:

\begin{equation}
\mathbf{J}_i(\qvec) = \frac{\partial}{\partial \qvec} T^{W}_{L_i}
\end{equation}

\noindent is called the link's \textit{kinematic Jacobian}, and for simple kinematic chains it can be computed in closed-form efficiently. The Jacobian is an important object for our problem, because it allows us to map instantaneous changes of the camera's pose to instantaneous changes in the robot's joints. 

The robot can also control its joints. Control signals $\uvec \in \mathbb{R}^N$ are sent to the joints, causing them to move based on the robot's physical dynamics.

\sectionbf{Depth Cameras}
\label{sec:depth_cameras}
This work is primarily concerned with two kinds of sensors: 2D cameras and depth cameras. In both cases, the output is a 2D image. Call the image $I_D$, it is a function with  domain $\Omega \in \mathbb{R}^2$. The relationship between 3D points in the scene and 2D points on a camera image can be modeled using the simple pinhole camera intrinsic \cite{Hartley2004} model:

\begin{equation}
\Proj(x, y, z)  =\colvec{2}{u}{v} = \colvec{2}{\frac{f_x x}{z} + c_x}{\frac{f_y y}{z} + c_y}
\end{equation}

\noindent where $u, v$ are the 2D images coordinates, $x, y, z$ are the 3D point's coordinates in the camera's frame of reference (with $x$ to the right, $y$ down, and $z$ forward), and $f_x, f_y, c_x, c_y$ are the intrinsic parameters of the camera. The set of all points in 3D space which project onto the camera is called the \textit{field of view} of the camera, and is usually approximated by a truncated pyramid (or \textit{frustum}). We can also define the inverse projection model, which takes a camera coordinate $u, v$ and depth measurement $z$, and converts it into a 3D vector relative to the camera's focal point:

\begin{equation}
\InvProj(u, v, z) = z \colvec{3}{\frac{u - c_x}{f_x}}{\frac{v - c_y} {f_y}}{1}
\end{equation}

Depth cameras measure the range ($z$) at each image pixel to points in the scene. Projective depth cameras accomplish this by projecting a pattern of infrared light onto the scene and measuring the disparity of this pattern using an offset infrared camera. Time-of-flight cameras measure depth by directly shooting rays of infrared light at the scene and measuring the time it takes for the rays to reflect back to the sensor. In any case, the depth measured by depth cameras is noisy, incomplete, and contains systematic error.

\sectionbf{Articulated Tracking}
\label{sec:articulated_tracking}
In practice, robots cannot directly measure their configuration, instead they must rely on sensors (called \textit{joint encoders}) which indirectly measure the configuration of their joints. Between the robot's joint encoder and the actual degree of freedom of the joint, there may be mechanisms with unmodeled dynamics (such as non-rigid links, springs, gear trains, or pulley systems) which prevent the robot from knowing its own configuration with certainty. For example, each joint of the \textit{Baxter} robot consists of a motor with plastic gearing, followed by a spring, followed by a rotating mechanism. In the extreme case (\eg human pose tracking or hand tracking), some degrees of freedom may be completely unknown, and can only be inferred through external sensing.

Tracking articulated bodies with known kinematic structure using externally-mounted visual sensors is a well-studied topic in computer vision. For instance, commercial motion capture systems use markers (such as fiducials or reflective spheres) attached to articulated bodies along with an external camera to track the pose of human actors and objects in the scene. 

When only the kinematic structure of the body is known, but no markers are available, the problem more difficult due to the unknown correspondences between sensor measurements and the body itself (\ie the \textit{segmentation problem}). But even in this case, efficient solutions for tracking articulated bodies exist. In general, given a series of sensor measurements from timestep $1$ to $(t - 1)$, and control inputs

\begin{align}
Z_{1 \ldots t} &= \{Z_1, \ldots, Z_t\} \\
u_{1 \ldots t} &= \{\uvec_1, \ldots \uvec_t\}
\end{align}

\noindent the task is to estimate the configuration of the body at time $t$:

\begin{equation}
\qvec_t = \argmax_{\qvec} \Prob\big(\qvec | Z_{1\ldots t}, \qvec_{1 \ldots (t - 1)}, \uvec_{1 \ldots (t - 1)} \big)
\end{equation}

If the system is assumed to have the Markov property (\ie the current state is conditionally independent of everything given the prior state), the articulated tracking problem reduces to merely finding the next configuration of the body given its previous configuration and sensor measurement:

\begin{align}
\qvec_t &= \argmax_{\qvec} \Prob\big(\qvec | Z_{(t - 1)}, \qvec_{(t - 1)}, \uvec_{(t - 1)} \big) \\
                    &= \argmax_{\qvec} \frac{\Prob\big(Z_{(t - 1)} | \qvec, \qvec_{(t - 1)},  \uvec_{(t - 1)} \big) \Prob(\qvec | \qvec_{(t - 1)}, \uvec_{(t - 1)})}{\Prob(Z_{(t - 1)}, \qvec_{(t - 1)}, \uvec_{(t - 1)})} \\
                    &= \argmax_{\qvec}\bigg[ \log \Prob\big(Z_{(t - 1)} | \qvec, \qvec_{(t - 1)},  \uvec_{(t - 1)} \big) + \log  \Prob\big(\qvec | \qvec_{(t - 1)}, \uvec_{(t - 1)}\big)\bigg]
\end{align}

\noindent the term $\Prob\big(Z_{(t - 1)} | \qvec, \qvec_{(t - 1)},  \uvec_{(t - 1)} \big)$ represents the posterior probability that a sensor measurement was observed given some body configuration, while the term $\Prob\big(\qvec | \qvec_{(t - 1)}, \uvec_{(t - 1)}\big)$ represents the prior probability of the body's configuration given its previous configuration (called the \textit{motion model}). Articulated tracking approaches differ mainly in how they compute these two 	quantities, as well as how they maximize them. 

\begin{figure}
\includegraphics{img/graphical_model_tracking.pdf}
\caption{A graphical model of the articulated tracking problem. Notice that the graph has the Markov property.}
\label{fig:graphical_model_tracking}
\end{figure}

For instance, one method intended for tracking humans in 2D images, Articulated ICP \cite{Mundermann2007}, computes the posterior using image-based edge features, and maximizes the likelihood of a configuration by coordinate descent in configuration space. 

One of the author's earlier works, Real-time Markerless Articulated Tracking (RMAT)\cite{Klingensmith2013} tracks robots using a depth camera,  a simple posterior model that sensor measurements are near the robot's surface as projected onto the depth image, and a motion model which assumes the robot's configuration changes slowly between timesteps. Sensor measurements are matched to corresponding points on the robot's body using an octree, and gradient descent is used to maximize the likelihood of the robot's configuration given its previous configuration and depth image.

A related but seperate work, Dense Articulated Real-time Tracking (DART) \cite{Schmidt2014}, improves upon RMAT by using a signed distance field representation of the robot's body rather than an octree, a more complex motion model that considers joint velocity, and an extended Kalman filter for tracking. DART has been shown to effectively track robots, human hands, and rigid bodies in real-time with commercial depth cameras.

The articulated tracking problem is a subset of the problem this thesis will address. The requirement that the robot's sensor be able to see its own links greatly reduces the complexity of the problem, turning it into a tracking problem rather than a SLAM problem -- but the same techniques used to compute the robot's configuration from sensor measurements can be used even when the scene is unknown. 

\sectionbf{Simultaneous Localization and Mapping}
\label{sec:slam}
The Simultaneous Localization and Mapping (SLAM) problem \cite{Thrun2005} involves concurrently estimating the pose of a moving sensor and the structure of the world around it when both are unknown. That is, given a sequence of noisy sensor readings

\begin{equation}
Z_{1 \ldots t} = \{Z_1, Z_2, Z_3, \ldots, Z_t\} 
\end{equation}

\noindent estimate the pose of the sensor at all times, $H_{1 \ldots t} \in SE(3)$ and the scene $M$:

\begin{equation}
 H_{1..t}, M = \argmax_{{H, M}} \Prob\big(H, M \big| Z_{1\ldots t}\big)
\end{equation}

\begin{figure}
\includegraphics{img/graphical_model_slam.pdf}
\caption{A graphical model of the SLAM problem. Note that given a known map, it has the Markov property, and is just the tracking problem. Without a known map, all measurements are correlated.}
\label{fig:graphical_model_slam}
\end{figure}

\subsection{Sparse SLAM}
\label{sec:sparse_slam}
Depending on the scene representation and sensor type, different approaches to solving this problem are appropriate. If the measurement model of the sensor consists of a sparse set of features (such as SIFT features, corner features, or related), a \textit{sparse} landmark-based scene representation is most appropriate. Sparse SLAM techniques build a globally-consistent graph of tracked landmarks (called a \textit{pose graph}) with pairwise edges on sensor motion between frames; and then optimize this pose graph based on a consistency metric. Since only a very small subset of the sensor data is used, this method can be made to work efficiently over very long datasets featuring loop closures. Specifically, the typical sparse SLAM pose graph has the form:

\begin{equation}
M = \{V, E\}
\end{equation}

\noindent where $V = \{v_1, \ldots, v_N\}$ is a colletion of vertices, with $v_i \in SE(3)$ as either a landmark pose or a sensor pose, and $E=\{e_1, \ldots, e_M\}$ is a collection of directed edges, with each edge defining a transformation $e_i = T_{v_a}^{v_b}$ from one vertex $v_a$ to another vertex $v_b$. A pose graph is said to be \textit{consistent} whenever each edge is consistent (\ie, whenever $v_a e_i  = v_b$). A \textit{loop closure} happens whenever two edges share the same outgoing vertex.

\begin{figure}
\includegraphics{img/pose_graph.pdf}
\caption{An example of a pose graph.}
\end{figure}

This breaks up the graphical model (\figref{fig:graphical_model_slam}) so that sensor measurements are only corellated by shared landmarks, rather than all sensor measurements being corellated.

\subsection{Dense SLAM}
\label{sec:dense_slam}
However, our use case calls for a robot to localize itself only in a very small workspace, and to use the reconstructed scene for geometric reasoning about objects for the purpose of manipulating them. For this use case, it is much more important to have a detailed geometric representation of the scene than it is to have consistent pose over very long distances. 

Dense SLAM techniques, in contrast to sparse SLAM techniques, use all or most of the sensor data to reconstruct a geometric model of the scene. This technique ensures that at all timesteps, a plausible geometric model taking into account all of the sensor data exists -- but consequently, the memory required to store the map at all timesteps is much larger than in landmark-based SLAM. Some dense methods (such as \textit{point fusion}\cite{Keller2013}) still manage to store a globally consistent pose graph as well as a geometric model by sacrificing some model quality, while most others (such as \textit{kinect fusion}\cite{KinectFusion} and variants) store the map and pose only for time $t - 1$, and then maximize:

\begin{align}
M_{t - 1} &= \argmax_{M}\Prob\big(M | Z_{t - 1}, H_{t - 1}\big) \\
H_t  &= \argmax_{H}\Prob\big(H | H_{t - 1}, M_{t - 1}\big) 
\end{align}

\noindent before generating the current model $M_{t}$ from the previous model $M_{t - 1}$ and current pose  $H_{t}$, and so on. By only storing a single map, these dense techniques avoid having to store dense models for every frame. This simplification ignores the fact that the unknown map correlates all of the sensor readings together (\figref{fig:graphical_model_slam}), but results in systemic errors in pose/map estimation building up over time.

A dense map representation of particular interest to our problem is the signed distance field (SDF) \cite{Curless1996}. The SDF stores a function that gives the distance to the nearest surface in the scene:

\begin{equation}
\Phi(\mathbf{x}) = \min_{\mathbf{s} \in S} \| \mathbf{x} - \mathbf{s} \| \mathbf{I}(\mathbf{s})
\end{equation}

\noindent where $S \subseteq \mathbb{R}^3$ is the set of all occupied points in the scene, and 

\begin{equation}
\mathbf{I}(\mathbf{s}) = 
\begin{cases}
-1 &~\text{if} ~\mathbf{s}~\text{is inside an object} \\
+1&~\text{otherwise}
\end{cases}
\end{equation}


\noindent $\Phi$ is negative inside objects, positive outside objects, and zero along the surface. The gradient $\nabla \Phi$ can be easily calculated,  and the representation very flexible, allowing $\Phi$ to represent many kinds of scenes. On the other hand, storing the SDF requires memory on the order of the volume of the scene.

\begin{algorithm}
\tcp{Given a depth image, sensor pose, previous SDF, previous voxel weights, a weighting function, a volume of interest, and a truncation distance}
\KwIn{$I_D, H_t, \Phi_{t - 1}, W_{t - 1}, w,  V, \tau$}

$H' \gets {H_t}^{-1}$

\tcp{Initialize the new distance field and voxel weights}
$\Phi_{t} \gets \Phi_{t - 1}$

$W_{t} \gets W_{t - 1}$

\tcp{For each voxel center in a volume of interest}
\For{$\mathbf{v} \in V$}
{
    \tcp{Transform the voxel to the camera frame and project it on the depth image.}
    $\mathbf{v}_h \gets H' \mathbf{v}$
	
	$d_i \gets I_D\left[\Proj(\mathbf{v}_h)\right]$
	
	\tcp{The geometric depth of the voxel to the camera plane.}
	$d_v \gets \mathbf{v}_{h}(z)$
	
	\tcp{The difference between the geometric depth and the reported depth is a local approximation of the SDF}
	$u \gets d_v - d_i$
	
	\tcp{If the local approximation is within the truncation distance\ldots}
	\If{$|u| < \tau$}
	{
	    \tcp{Compute the weighted average with the previous SDF.}
		$\Phi_{t}(\mathbf{v}) \gets \frac{W_{t}(\mathbf{v}) \Phi_{t}(\mathbf{v}) + w(u) u}{W_{t}(\mathbf{v}) + w(u)}$
		
		$W_{t}(\mathbf{v}) \gets W_t(\mathbf{v}) + w(u)$
	}
}
 
\KwOut{$\Phi_{t}, W_{t}$}
\caption{\textsc{FuseTSDF}}
\label{alg:TSDF}
\end{algorithm}

SDF-based dense SLAM techniques (such as \textit{kinect fusion} and variants) build $\Phi$ at time $t$ by fusing the depth image produced by the camera at time $t - 1$. A depth image can be fused into an SDF by computing a locally-linear approximation of the SDF for the depth image, and averaging it into the SDF computed at $t - 1$.  Since only one dense model is maintained at each timestep, errors in the model can compound over time. In practice, the SDF is computed only for a small region around the surfaces of objects (called the \textit{truncation region}) for which the locally-linear approximation is good. The result is the truncated signed distance field (TSDF\cite{Curless1996}) (\algref{alg:TSDF}).

One of the author's earlier works, \textsc{Chisel}\cite{Klingensmith2015}, explores techniques for efficiently building and storing the SDF used in dense SLAM, and extends the technique to mobile phones. This thesis will be using \textsc{Chisel} as a backend dense mapping technique.

\section{Direct Monocular SLAM}
\label{sec:direct_slam}
For monocular cameras, techniques exist which combine the benefits of sparse and dense SLAM. These techniques (called direct, or direct semi-dense techniques) track the pose of a moving camera by directly minimizing the \textit{photometric error} between subsequent camera frames. This involves directly estimating the depth of pixels in the 2D camera image and reprojecting pixels from one frame onto another, and then maximizing the likelihood of the subsequent camera poses:

\begin{equation}
H_{t} = \argmin_{H} \big\| I_{t - 1}[Z_{t - 1}] - I_{t}[\Proj(H Z_{t - 1})]\big\|
\end{equation}

\noindent where $I$ is the camera image at a given time, and $Z_t$ is the point cloud as inferred by stereo matching between pose $H_t$ and $H_{t - 1}$. Examples of  direct techniques include \textsc{Orb-Slam}\cite{Murartal2015}, \textsc{Lsd-Slam}\cite{Engel2015}, and \textsc{Dtam} \cite{Newcombe2011}.

Direct monocular methods, like sparse keypoint methods, are scale ambigous, and cannot distinguish between two scenes that differ only in scale. Like sparse keypoint methods,  they generally build a pose graph on top of the dense or semi-dense localization system.

Unlike sparse keyframe-based algorithms, semi-dense direct techniques are capable of creating high-quality, dense 3D reconstructions. Unlike purely dense algorithms, they do not require a depth image to work. These qualities make semi-dense techniques attractive for this thesis. 

\chapter{Framework}
\label{chap:framework}
\sectionbf{Articulated SLAM}
\label{sec:kin_slam}
When the sensor mounted to the robot is outward-facing, and nothing is known about the world around the robot, how can the robot simultaneously estimate its configuration, and reconstruct the 3D geometry of the world? This is equivalent to the articulated tracking problem with an unknown world model, and also equivalent to the visual SLAM problem with additional constraints on the pose of the sensor.

Given a series of sensor measurements $Z_{1}, \ldots, Z_{t}$ and control inputs $\uvec_{1}, \ldots, \uvec_{t}$, the algorithm must simultaneously estimate the trajectory of the robot's configuration $\qvec_1, \ldots \qvec_{t}$, and a dense model of the world $M$:

\begin{equation}
\qvec_1, \ldots, \qvec_t, M = \argmax_{\mathbf{q_{\ldots}}, M} \Prob\big(\qvec_{\ldots}, M  | Z_{1 \ldots t}, \uvec_{1 \ldots t} \big)
\label{eqn:kin_slam}
\end{equation}

\begin{figure}
\includegraphics{img/graphical_model_constrained_slam.pdf}
\caption{A graphical model of the articulated SLAM problem. It combines aspects of the articulated tracking problem and the SLAM problem..}
\label{fig:graphical_model_constrained_slam}
\end{figure}

This problem is made difficult by the fact that the unknown map $M$ correlates all of the sensor measurements together (\figref{fig:graphical_model_constrained_slam}). First, consider the articulated tracking problem with a \textit{known} map. In this case, \eqnref{eqn:kin_slam} reduces to

\begin{equation}
\qvec_1, \ldots, \qvec_t= \argmax_{\mathbf{q_{\ldots}}} \Prob\big(\qvec_{\ldots} |M,  Z_{1 \ldots t}, \uvec_{1 \ldots t} \big)
\label{eqn:kin_tracking}
\end{equation}

This becomes a state estimation problem rather than a full SLAM problem, and therefore acquires the Markov property. So it suffices to estimate the current state given the previous state estimate only:

\begin{align}
\qvec_t &= \argmax_{\qvec} \Prob\big(\qvec | \qvec_{t - 1}, \uvec_{t - 1}, M, Z_{1 \ldots t}\big) \\
                    &= \argmax_{\qvec} \frac{\Prob\big( Z_{1 \ldots t} | \qvec, \qvec_{t - 1},  \uvec_{t - 1}, M  \big) \Prob \big( \qvec |\qvec_{t - 1}, \uvec_{t - 1}, M \big)}{\Prob\big(Z_{1 \ldots t},\qvec_{t - 1}, \uvec_{t - 1}, M\big)} \\
                    &= \argmax_{\qvec} \log \Prob\big(Z_{1 \ldots t} | \qvec, \qvec_{t - 1}, \uvec_{t - 1}, M \big) + \log \Prob \big( \qvec | \qvec_{t - 1}, \uvec_{t - 1}, M\big) \\
                    &= \argmax_{\qvec} \log \Prob\big(Z_t | \qvec, M\big) + \log \Prob \big (\qvec | \qvec_{t - 1}, \uvec_{t - 1})
\end{align}

\noindent yielding two terms which must be maximized: a posterior term $\Prob \big(Z_t | \qvec, M\big)$, representing the likelihood of the sensor measurements given a robot configuration and known map,  and a prior term $\Prob \big (\qvec | \qvec_{t - 1}, \uvec_{t - 1})$, representing the likelihood of a robot configuration given its past configuration and control input. 

\section{Prior Motion Model}
\label{sec:motion_prior}
The prior $\Prob \big (\qvec | \qvec_{t - 1}, \uvec_{t - 1})$ depends on the robot's dynamics, and its relationship to gravity and other external forces. In general, the forward dynamics model of a robot arm can be expressed using the well-known differential equation based on its Lagrangian:

\begin{equation}
\tau(\uvec) = \mathbf{M}(\qvec)\ddot{\qvec} + \mathbf{C}(\qvec, \dot{\qvec})\dot{\qvec} + \mathbf{G}(\qvec, \dot{\qvec}, F_{\text{ext}})
\end{equation}

\noindent where $\tau$ is the torque the robot produces at its joints given $\qvec$, $\mathbf{M}$ is a function representing the robot's mass projected into joint space, $\mathbf{C}$ is a function representing the internal corolis forces that the robot experiences, and $\mathbf{G}$ is a function representing how the robot responds to external forces $F_{\text{ext}}$ (usually gravity). 

If this dynamics model were known perfectly, then the prior motion model would be completely deterministic, and indeed, no external sensors of any kind (not even joint encoders) would be needed to track its state over time. Unfortunately, the dynamics model of a robot manipulator is not easy to calculate, and depends on having a full account of external and internal forces the robot experiences. 

These unmodeled dynamics can be mitigated in two ways: first, by treating them like noise in the motion model, and second, by using external sensors to account for them. At the very least, the dynamic model of the robot provides a narrow region of configuration space that the robot's configuration can be in, given its previous configuration and control input.

\section{Motor Encoder Posterior}
\label{sec:encoder_noise}
\begin{marginfigure}
\includegraphics{img/motor_model.pdf}
\caption{There is always some unknown mechanism between the method of actuation (a motor), and the physical robot degree of freedom. }
\label{fig:motor_encoder}
\end{marginfigure}

\begin{marginfigure}
\includegraphics{img/encoder_noise.pdf}
\caption{A sketch of a hypothetical relationship between the motor encoders $\qvec^{(e)}$  and the physical joint angles $\qvec^{(j)}$, note that the relationship is multivariate, even though it is shown as one-dimensional here.}
\label{fig:encoder_noise}
\end{marginfigure}

Robots invariably have sensors (called \textit{encoders}) mounted on their motors and/or joints. These sensors report a value $\qvec^{(e)}$ that correlates to the actual geometric joint angle  $\qvec^{(j)}$, but, due to the complications of an unknown intervening mechanism (\figref{fig:motor_encoder}), the encoder's reported value may have noise and systematic error (\figref{fig:encoder_noise}). For instance, the Barrett \textit{WAM} robot arm is cable-driven. Its' encoders are attached to its' motors, and a complicated system of cables and pulleys transmits torque from the motor to the joint. Unmodeled dynamics from stretch in the cable system leads to a discrepancy between $\qvec^{(e)}$ and $\qvec^{(j)}$.

Define a simple probabalistic model:

\begin{equation}
\qvec^{(j)} = g( \qvec ^{(e)}) + \epsilon (\qvec^{(e)}) 
\label{en:joint_prior}
\end{equation}

\noindent where $g : \mathbb{R}^N \to \mathbb{R}^N$ is a static calibration function that maps motor encoders to joint angles, and $\epsilon(\qvec^{(e)})$ is a random variable capturing the variance of the mapping from the motor encoders to joint angles (\figref{fig:encoder_noise}). This model may be learned by Gaussian Process Regression, or other regression techniques, when ground truth data is known.

Initial tests carried out prior to this proposal on a Barret \textit{WAM} arm using optical joint encoders as ground truth have revealed that the mapping (\eqnref{en:joint_prior}) between the motor encoders and joint angles depends not only on the configuration of the robot, but also on the applied torque, the path that the robot has taken to get to a particular configuration, and other variables that are not easy to capture using a simple Gaussian model. 

\section{Depth Camera Posterior}
\label{sec:depth_cam_posterior}
The sensor posterior term, $\Prob \big(Z_t | \qvec, M\big)$, represents the likelihood of sensor measurements given a particular robot configuration $\qvec$ and dense map $M$. Here, the relevant work from dense 3D SLAM can be taken into account. Consider a depth camera mounted on the robot's end effector which can see portions of the dense map. The depth data can be modeled as cones, rays, or points. Since each depth pixel measurement is independent given the map and robot configuration, the likelihood of each ray, cone, or point, can be calculated independently.

~\\

\begin{figure}
\includegraphics{img/pointcloud.pdf}
\caption{A depth camera with focal point $\mathbf{o}$ and depth image $I_D$ projects points $\mathbf{z}_{u, v}$ into the scene.  All the points are expected to lie along surfaces where the signed distance to surfaces in the scene $\Phi = 0$.} 
\label{fig:pointcloud}
\end{figure}


\noindent \textbf{Point Cloud Model}
\label{subsec:point_cloud}
One of the simplest means of modeling the depth camera is to unproject each depth pixel into the scene as a point using the camera intrinsics. The result is the unstructured \textit{point cloud} of the depth image (\figref{fig:pointcloud}). So  the sensor measurement is:

\begin{equation}
Z_t = \{H_t ~\InvProj\left(I_D [u, v]\right) = \mathbf{z}_{u, v} ~\forall [u, v] \in \Omega\}
\end{equation}

\noindent and due to the conditional independence of the depth image given a robot configuration and map, the posterior is:

\begin{equation}
\Prob \big(Z_t | \qvec, M\big) = \prod_{u, v \in \Omega} \Prob\big(\mathbf{z}_{u, v} | \qvec, M\big)
\end{equation}

Assume that the points in the point cloud are corrupted by simple isotropic Gaussian noise. If this is the case, 

\begin{equation}
\Prob\big(\mathbf{z}_{u, v} | \qvec, M\big) \propto \exp\left(-\Phi(\mathbf{z}_{u, v})^2\right)
\end{equation}

\noindent where $\Phi$ is the signed distance field representation of the scene $M$.

This model, while simple, ignores occlusions. It makes no distinction between points behind obstacles and points in front of obstacles from the perspective of the camera. It also ignores the aniostropic (direction-dependant) nature of depth sensor noise. A more complicated model may be needed to correctly capture the depth camera posterior.

~\\

\begin{figure}
\includegraphics{img/raycone.pdf}
\caption{A cone/ray model of the depth camera. The cone emenates from the sensor's focal point $\mathbf{o}$, with direction $\hat{n}$. It hits the surface of the scene at point $\mathbf{\mathbf{z}_uv}$, which is distance $z$ along its center.} 
\label{fig:raycone}
\end{figure}

\noindent\textbf{Ray/Cone Cloud Model}
\label{subsec:raycone}
The depth camera can be modeled as a set of cones emenating from the camera's focal point -- one cone for each depth pixel. The cone is the swept volume of the depth pixel projected into the scene, and has a length equal to the depth pixel's measurement (\figref{fig:raycone}). The cone may be seen as the union of all rays that pass through the depth pixel from the focal point of the camera.

This is a much more complex model, incorporating information about which points in the scene have been passed through by rays as well as which points lie on surfaces (as well as their surface normal). 

\textbf{TODO: How to proceed with the ray model? I can make reference to my voxel carving work, and probably to DART, which has a beautiful symmetric interpretation of the passthrough information.}

\sectionbf{Dense Sensor-To-Map Constrained SLAM}
\label{sec:dense_constrained_slam}
Given a model of the sensor posterior and motion model, and a dense signed distance field representation of the map, it is possible to formulate a simple dense \sref{sec:dense_slam} sensor-to-map algorithm for articulated SLAM. Using the point cloud posterior model \sref{subsec:point_cloud}, and a simple motion model which just assumes that the difference between the robot's configuration and the reported motor encoder readings \sref{sec:motion_prior}  $\|\qvec_t - \qvec_t^{e}\|$ is small, the problem can be treated as a simple optimization problem at each timestep.

\begin{align}
\qvec_t &\gets \argmax_{\qvec}\Prob\big(\qvec | Z_{t}, M_{t - 1}, \qvec_{t - 1}\big) \\
\label{eq:simple_posterior}
M_t &\gets \argmax_{M}\Prob\big(M | \qvec_{t}, Z_{t}, M_{t - 1}\big)
\end{align} 

The configuration update (\eqnref{eq:simple_posterior}) can be maximized by breaking it into its component parts

\begin{equation}
  \argmax_{\qvec}\Prob\big(\qvec | Z_{t}, M_{t - 1}, \qvec_{t - 1}\big) = 
  \argmax_{\qvec} \log \Prob\big(Z_t | \qvec, M_{t - 1}\big) + \log \Prob \big (\qvec | \qvec_{t - 1})
\end{equation}

\noindent and the sensor posterior from the point cloud model can be computed as:

\begin{align}
\log \Prob\big(Z_t | \qvec, M_{t - 1}\big) &=  \log \prod_{[u, v] \in \Omega} \Prob(\mathbf{z}_{u, v} | \qvec, M_{t - 1}) \\
&\propto \sum_{[u, v] \in \Omega} \log \exp\left(-\Phi(\mathbf{z}_{u, v})^2\right) \\
&= \sum_{[u, v] \in \Omega} -\Phi(\mathbf{z}_{u, v}) ^2
\end{align}

\noindent where $\mathbf{z}_{u, v} = H_t ~\InvProj\left(I_D [u, v]\right)$ is a point from the point cloud of the depth image at time $t$, and $\Phi$ is the signed distance field of the map at time $t - 1$.

So, maximizing the sensor posterior is equivalent to minimizing the simple cost function:

\begin{equation}
c(\qvec) =  \frac{1}{2}\sum_{[u, v] \in \Omega}\Phi(\mathbf{z}_{u, v}) ^2
\end{equation}

\noindent and $c$ is minimized whenever $\frac{\partial c}{\partial \qvec} = 0$. That is:

\begin{align}
\nabla c &=  \frac{1}{2}\sum_{[u, v] \in \Omega}\left[\frac{\partial}{\partial \qvec}   \Phi(\mathbf{z}_{u, v}) ^2 \right]\\
 &= \sum_{[u, v] \in \Omega} \left[\Phi(\mathbf{z}_{u, v}) \frac{\partial}{\partial \qvec} \nabla \Phi(\mathbf{z}_{u, v})\right] \\
 &= \sum_{[u, v] \in \Omega} \left[\Phi(\mathbf{z}_{u, v}) \mathbf{J}(\qvec, \mathbf{z}_{u, v})^T\nabla \Phi(\mathbf{z}_{u, v})\right]
\end{align}

\noindent where $ \mathbf{J}(\qvec, \mathbf{z}_{u, v}) \in \mathbb{R}^{3 \times N}$ is the manipulator's kinematic jacobian\sref{sec:robot_kinematics} evaluated for the point $\mathbf{z}_{u, v}$ as rigidly attached to the end effector. The gradient of the SDF $\nabla \Phi$ is easy to approximate using finite differencing.

\begin{figure}
\includegraphics{img/robot_reconstruct.pdf}
\caption{A rendering of a robot manipulator with attached depth sensor. Points $\mathbf{z}_{u, v}$ are projected into the scene, which is compared with the map's distance field from the previous timestep $\Phi$. The gradient of $\Phi$ at each point in the point cloud is shown as an arrow. The end effector transform at time $t$ is given by $H_t$, the robot's configuration at that time is $\qvec_t$.} 
\label{fig:robot_reconstruct}
\end{figure}

This cost function and gradient has an interesting physical interpretation (\figref{fig:robot_reconstruct}). If all of the points in the point cloud are interpereted as being rigidly attached to the camera frame, and a force is applied at each of the points $\mathbf{z}_{u, v}$ with a force in the direction $\nabla\Phi(\mathbf{z}_{u, v})$ with magnitude $\Phi(\mathbf{z}_{u, v})$, the gradient $\nabla c$ is the same as the instantaneous acceleration the joints would experience from the sum of such forces.

The likelihood of $\qvec_t$ can be maximized using gradient descent of the cost function $c$ regularized by the motor encoders (\algref{alg:gradient_descent}). In this way, the robot's configuration stays near what is reported by its motor encoders, but is allowed to change in response to the sensor data.

\begin{algorithm}

\tcp{Where $\qvec_t^{(e)}$ are the motor encoders at time $t$,  $\lambda$ is a learning rate, and $\gamma$ is a regularization parameter.}
\KwIn{$Z_{t}, \qvec_t^{(e)}, \qvec_{t - 1}, \Phi_{t - 1}, \lambda, \gamma$}

\tcp{Initialize configuration offset $\epsilon \in \mathbb{R}^N$ to zero.}
$\epsilon \gets \mathbf{0}$

\Repeat{\text{convergence}}
{
    \tcp{Current estimate for $\qvec_t$ is the joint encoders plus the offset}
	$\qvec_t \gets \qvec_t^{(e)} + \epsilon$
	
	\tcp{Compute the gradient of the sensor measurement posterior}
	$\nabla c (\qvec_t) \gets  \sum_{[u, v] \in \Omega} \left[\Phi(\mathbf{z}_{u, v}) \mathbf{J}(\qvec_t, \mathbf{z}_{u, v})^T\nabla \Phi(\mathbf{z}_{u, v})\right]$
	
	\tcp{Descend the gradient regularized by $\gamma$}
	$\epsilon \gets \epsilon - \lambda \nabla c (\qvec_t) - \gamma \epsilon$
}

\tcp{Now that there is a new estimate of $\qvec_t$, fuze the sensor data into the SDF.}
$\Phi_{t} \gets \textsc{FuseTSDF}(Z_{t}, F(\qvec_t), \Phi_{t - 1}, \ldots)$
 
\KwOut{$\qvec_t, \Phi_{t}$}

\caption{Regularized Dense SDF Gradient Descent.}
\label{alg:gradient_descent}
\end{algorithm} 

\chapter{Experiments}
\label{chap:experiments}
\section{2D Simulation Experiments}
The constrained descent algorithm (\algref{alg:gradient_descent}) can be illustrated by constructing a simple 2D simulation (\tableref{table:2d_experiments}). In the simulation, a 3-link serial robot manipulator with a simulated 1D depth sensor scans a scene. Zero-centered Perlin \cite{Perlin02} noise is added to its joint encoder readings. That is,

\begin{equation}
\qvec^{(e)}_{t} = \beta_n\textsc{Perlin}\big(s_n \qvec^{(j)}_{t}\big)
\end{equation}

\noindent where $s_n$ is a noise scale parameter, and $\beta_n$ is a noise magnitude parameter.  

For the world model, a simple 2D truncated signed distance field (TSDF) is constructed. The performance of the constrained gradient descent algorithm (\algref{alg:gradient_descent}) is compared against a simple unconstrained descent algorithm which assumes the sensor can move and rotate freely, without considering the robot kinematics.

\begin{figure}
\includegraphics[width=1.0\textwidth]{img/2d_experiments/ee_error.pdf}
\includegraphics[width=1.0\textwidth]{img/2d_experiments/class_error.pdf}
\includegraphics[width=1.0\textwidth]{img/2d_experiments/tsdf_error.pdf}
\caption{2D experiment (\tableref{table:2d_experiments}) data. Here, end effector error is the squared distance between the reported end effector position and ground truth. The model classification error is the proportion of TSDF voxels that are misclassified (full vs. empty), and the total TSDF error is the total squared difference between the ground truth distances and the reported distances.}
\label{fig:2d_experiment_data}
\end{figure}


The robot scans the scene, and makes a single loop closure. When using the (noisy) joint encoders for pose only, much of the detail of the scene is lost, since the TSDF ($\Phi$) smooths out errors through averaging. Constrained gradient descent allows the robot to correct its joint encoder error and recapture fine details in the scene (though drift is still made apparent by the loop closure). Unconstrained gradient descent, on the other hand, causes the robot's estimate of the sensor pose to drift; and the 3D reconstruction is quickly destroyed as error accumulates (\figref{fig:2d_experiment_data}).

\newcommand{\tabfigure}[1]{ \raisebox{-.5\height}{\includegraphics[width = 3.5cm, height = 3.5cm]{img/2d_experiments/2d_slam_#1.png}}}
\newcommand{\tabrow}[1]{#1 & \tabfigure{groundtruth/#1} & \tabfigure{encoders/#1} & \tabfigure{constrained/#1} & \tabfigure{unconstrained/#1}} 
\begin{table*}
\caption{}
\label{table:2d_experiments}  
\begin{tabular}{ccccc}
\toprule
\bfseries Time & \bfseries Ground Truth & \bfseries Encoders & \bfseries Constrained & \bfseries Unconstrained \\
\midrule
\tabrow{250}\\
\tabrow{500} \\
\tabrow{750} \\
\tabrow{1000}\\
\tabrow{2000}\\ 
\end{tabular}
\\ 
\smallskip\noindent\small Table \ref{table:2d_experiments} :
{2D experiments. A 3-joint robot arm (dark red) scans the scene with a simulated depth camera (light grey lines). Its joint encoders are corrupted by noise (green). It builds up a TSDF, $\Phi$ (red and blue pixels) of the scene (black pixels). The ground truth is compared to just using the robot encoders, constrained descent (\algref{alg:gradient_descent}), and unconstrained descent over time.}
\end{table*}  

\begin{marginfigure}
\includegraphics{img/arm_track_3d_sim.png}
\caption{A 3D simulation environment featuring a robot mounted to a table, looking at a bookshelf.}
\label{fig:arm_track_3d_sim}
\end{marginfigure}

\section{3D Simulation Experiments}
The same concept can be extended to 3D scenes (\figref{fig:arm_track_3d_sim}). We built a framework for doing dense sensor-to-model articulated tracking (\algref{alg:gradient_descent}) in 3D, as well as a depth sensor simulator. Like in the 2D experiments, the robot follows a fixed trajectory. Perlin noise is added to the joint encoders. The robot builds a 3D \textsc{TSDF} using the simulated depth data (\figref{fig:sim_depth}). \figref{fig:sim_3d_reconstructions} shows the result of the same robot trajectory and depth data being used to reconstruct a bookshelf without any joint encoder noise, with noise and dense articulated 3D tracking, and without any tracking. Even under conditions of extreme encoder noise, dense frame-to-model SLAM is able to create a high-qaulity 3D reconstructions of the scene. On the other hand, using the noisy encoders alone, the 3D reconstruction is almost totally unrecognizable.

\begin{figure}
\caption{ }
\label{fig:sim_3d_error}
\includegraphics{img/3d_end_error.pdf}
\smallskip\noindent\small {Figure \ref{fig:sim_3d_error}: The end effector error in the 3D simulation experiment.}
\end{figure}

\begin{marginfigure}[-6.5in]
\includegraphics{img/sim_depth.png}
\caption{Simulated depth camera.}
\label{fig:sim_depth}
\end{marginfigure}

\begin{marginfigure}[-2.5in]
\centering
\includegraphics[width=1.0\textwidth]{img/groundtruth_bookshelf.png} Ground Truth
\includegraphics[width=1.0\textwidth]{img/tracked_bookshelf.png} Noisy Encoders + Tracking
\includegraphics[width=1.0\textwidth]{img/odom_bookshelf.png} Noisy Encoders Only
\caption{Simulated 3D \textsc{TSDF} reconstructions.}
\label{fig:sim_3d_reconstructions}
\end{marginfigure}

\figref{fig:sim_3d_error} shows the translation error of the end effector vs. ground truth during tracking. The end effector error consistently converges to be beneath 2 cm even when the error due to noisy joint encoders is more than 11 cm. 

The deficiencies of the simple gradient descent algorithm can be seen around timestep 500, 3000, etc. where the noisy encoders have better end effector accuracy than the tracking. This is because the prior motion model is too simple to account for high-frequency noise. 

\chapter{Research Questions}
\label{chap:research}
This thesis proposal has introduced a novel framework for doing 3D visual SLAM in articulated systems; but much further research is needed to investigate the theoretical grounding and concrete properties of this framework. Here are a few questions which must be answered by the thesis:

\sectionbf{What applications are enabled by articulated SLAM?}

Building high quality 3D reconstructions is one thing, but what can be done with the output of the articulated SLAM system? One possibility is to use the model to plan manipulation strategies (such as grasping and reaching into tight spaces), or to control the arm (\ie visual servoing). When the robot is no longer passively observing the model of the world, but instead can control where it places the sensor, the whole field of active perception opens up. How should the robot move to best reconstruct the scene?

As interesting as these questions are, they will have to be secondary to the fundamental question of how to get the SLAM system to work in the first place. 

\sectionbf{How can the encoder noise be modeled on real robot arms?}

So far, we have only considered vague properties of encoder noise \sref{sec:encoder_noise} -- that it is really unmodeled dynamics causing the discrepency between joint encoders and the actual joint angles, and that it is likely configuration-dependant. In simulated experiments, we have modeled the noise using a random Perlin function \sref{chap:experiments}. But what exactly \textit{are} the unmodeled dynamics we are trying to capture? What physical processes cause them, and how can these physical processes be modeled?

Answering this question will require a literature review of existing methods, data collection on real robots, and a framework for testing different dynamics models.

\sectionbf{What are the limitations of dense camera-to-model SLAM using real data/robots?}

So far, our experiments have only covered 2D and 3D simulated datasets. Obviously, real robots have unique challenges not addressed by the simulations: extrinsic/intrinsic calibration error, sensor noise, unmodeled dynamics, etc. We are eager to test articulated SLAM on a real robot. Experiments must be devised to test the convergence basin of the SLAM system. One particularly challenging component will be getting ground-truth data for the pose of the robot and geometric structure of the world (which are easy to get in simulation).

\sectionbf{How can a pose graph be incorporated into the articulated SLAM framework?}

So far, we have only derived dense camera-to-model 3D SLAM \sref{sec:dense_constrained_slam}. This approach works well for small workspaces when the displacement between camera poses is small, but fails over long trajectories and large workspaces. The camera-to-keyframe or pose graph methods \sref{sec:sparse_slam} correct for drift by globally optimizing the entire trajectory with respect to every keyframe. Incorporating a pose graph into the articulated SLAM problem would be an ideal way of correcting for drift.

Answering this question will require re-deriving pose graph optimization in terms of \textit{robot configurations} rather than \textit{camera poses}.

\sectionbf{How can direct monocular articulated SLAM be accomplished?}

So far, we have only discussed solutions for dense camera-to-model 3D SLAM, which requires knowledge of a depth image at all time steps. How can we reconstruct a scene and estimate the robot's configuration when only a \textit{monocular} camera is available? Direct or semi-direct \sref{sec:direct_slam} SLAM is an attractive set of solutions for  3D reconstruction  using monocular cameras. Such methods rely on direct optimization in $SE(3)$ of camera poses given reprojected pixel intensities. 

Answering this question will require constructing a direct image residual optimization technique in the configuration space of the robot, instead of in $SE(3)$. That is, we will have to derive the direct relationship between \textit{pixel intensities} and \textit{joint angle displacements}. 

\sectionbf{Why articulated SLAM?}

This is perhaps the biggest research question. What advantages does an articulated SLAM system have over a traditional unconstrained 6DOF SLAM system? We believe that doing SLAM directly in the natural space of the robot arm (its configuration space) is a more accurate and theoretically grounded way of solving the problem -- but data will have to be collected to verify this, comparing existing SLAM systems with their articulated counterparts.


\chapter{Timeline}
I want to complete this thesis work in one year. I will be focusing my efforts on two major conference papers: one for ICRA 2016, and one for IROS 2016. Other conferences may be selected depending on how my research is going.

\begin{table*}
\begin{tabular}{lll}
\toprule 
\large Date &   & \large Conference \\
\midrule 
July 11, 2015 & Present \textsc{Chisel} at RSS. & RSS 2015\\
\hdashline[0.5pt/5pt]
August 2016 & Real experiments with dense frame-to-model tracking. & \\
\hdashline[0.5pt/5pt]
Sep 15, 2015 & Write paper on dense frame-to-model tracking.  & ICRA 2016  \\
\hdashline[0.5pt/5pt]
Fall 2015 & Begin implementing direct monocular system. & \\
\hdashline[0.5pt/5pt]
Mar 1, 2016 & Write paper on direct monocular system. &  IROS 2016 \\
\hdashline[0.5pt/5pt]
Spring 2016 & Implement pose graph. \\
\hdashline[0.5pt/5pt]
June 8, 2016 & Finish and defend this thesis & \\
\bottomrule 
\end{tabular}
\end{table*}

The first paper will focus on dense camera-to-model SLAM \sref{sec:dense_constrained_slam}, and will theoretical ground it, and provide results against other (non-articulated) SLAM algorithms. The second paper will focus on a direct monocular method \sref{sec:direct_slam}. Finally, I will look at incorporating a pose graph \sref{sec:sparse_slam}. Work from these conference papers will be incorporated into the thesis.

\backmatter

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliography{ArmSlam} % Use the bibliography.bib file for the bibliography
\bibliographystyle{plainnat} % Use the plainnat style of referencing

%----------------------------------------------------------------------------------------

%\printindex % Print the index at the very end of the document

\end{document}